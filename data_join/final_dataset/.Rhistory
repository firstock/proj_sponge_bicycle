tweet <- read.csv("bycle_final.csv")
#특수문자 삭제 @ 같은
# ?str_replace_all
# install.packages("stringr")
library(stringr)
tweet$text<-str_replace_all(tweet$text,"\\W"," ")
head(tweet$text)
# install.packages("KoNLP") #형태소 분석 패키지
# install.packages("wordcloud") #워드클라우드 패키지
# install.packages("RColorBrewer") #색상패키지
# Sys.setenv(JAVA_HOME='c:/Program Files/Java/jer1.8.0_161')
library(KoNLP)
library(wordcloud)
library(RColorBrewer)
useSejongDic()  #세종사전 활용하기
#명사추출
myword<-sapply(tweet$text, extractNoun, USE.NAMES = T)
myword
#list->unlist
gogo<-unlist(myword)
head(gogo)
#필요없는 단어 삭제하기
gogo<-gsub("\\d+","",gogo) #모든 숫자 없애기
gogo<-gsub("RT","",gogo)
gogo<-gsub("SSUL_","",gogo)
gogo<-gsub("최애에","",gogo)
gogo<-gsub(" ","",gogo)
gogo<-Filter(function(x){nchar(x)>1},gogo)
gogo<-sapply(gogo, function(x){Filter(function(y){nchar(y)<=8&&nchar(y)>1&&is.hangul(y)},x)})
#한글, 8자 이하만 필터링
#워드클라우드를 만들기 위해 단어만 수집된 텍스트 파일을 만드는 작업을 해준다
write(unlist(gogo),"screen_tweet.txt") #텍스트 파일로 저장
tweet_cloud<-read.table("screen_tweet.txt") #다시불러오기
head(tweet_cloud)
#table(분할표) 만들기
wordcount<-table(tweet_cloud) #단어카운트
install.packages("KoNLP") #형태소 분석 패키지
# install.packages("KoNLP") #형태소 분석 패키지
install.packages("wordcloud") #워드클라우드 패키지
# install.packages("KoNLP") #형태소 분석 패키지
install.packages("wordcloud") #워드클라우드 패키지
# install.packages("RColorBrewer") #색상패키지
# Sys.setenv(JAVA_HOME='c:/Program Files/Java/jer1.8.0_161')
library(KoNLP)
# install.packages("KoNLP") #형태소 분석 패키지
install.packages("wordcloud") #워드클라우드 패키지
# install.packages("KoNLP") #형태소 분석 패키지
# install.packages("wordcloud") #워드클라우드 패키지
install.packages("RColorBrewer") #색상패키지
install.packages("RColorBrewer")
# install.packages("KoNLP") #형태소 분석 패키지
# install.packages("wordcloud") #워드클라우드 패키지
install.packages("RColorBrewer") #색상패키지
install.packages("RColorBrewer")
# install.packages("KoNLP") #형태소 분석 패키지
# install.packages("wordcloud") #워드클라우드 패키지
# install.packages("RColorBrewer") #색상패키지
# Sys.setenv(JAVA_HOME='c:/Program Files/Java/jer1.8.0_161')
library(KoNLP)
library(wordcloud)
library(RColorBrewer)
useSejongDic()  #세종사전 활용하기
#명사추출
myword<-sapply(tweet$text, extractNoun, USE.NAMES = T)
myword
#list->unlist
gogo<-unlist(myword)
head(gogo)
#필요없는 단어 삭제하기
gogo<-gsub("\\d+","",gogo) #모든 숫자 없애기
gogo<-gsub("RT","",gogo)
gogo<-gsub("SSUL_","",gogo)
gogo<-gsub("최애에","",gogo)
gogo<-gsub(" ","",gogo)
gogo<-Filter(function(x){nchar(x)>1},gogo)
gogo<-sapply(gogo, function(x){Filter(function(y){nchar(y)<=8&&nchar(y)>1&&is.hangul(y)},x)})
#한글, 8자 이하만 필터링
#워드클라우드를 만들기 위해 단어만 수집된 텍스트 파일을 만드는 작업을 해준다
write(unlist(gogo),"screen_tweet.txt") #텍스트 파일로 저장
tweet_cloud<-read.table("screen_tweet.txt") #다시불러오기
head(tweet_cloud)
#table(분할표) 만들기
wordcount<-table(tweet_cloud) #단어카운트
wordcount<-gsub("민호","",wordcount) #필요없는단어 다시 삭제
wordcount<-gsub("염력","",wordcount) #필요없는단어 다시 삭제
#다시 파일 저장
write(unlist(gogo),"screen_tweet.txt")
tweet_cloud<-read.table("screen_tweet.txt") #다시 불러오기
wordcount<-table(tweet_cloud) #다시 table만들기
#워드클라우드생성
palete<-brewer.pal(10,"Set1") #색상 설정
wordcloud(names(wordcount),freq = wordcount, min.freq = 5, random.order=F, random.color=F, colors=palete)
# create word-document table
df2 <- read.csv("bycle.csv", stringsAsFactors = F)
str(df2)
head(df2)
colnames(df2)
rules <- sapply(df2$text, strsplit, " ", USE.NAMES = F)
# install.packages("tm")
library(tm)
reV <- unlist(rules)
revec <- VectorSource(rules)
revecco <- Corpus(revec)
rmyTdm <- TermDocumentMatrix(revecco, control = list(wordLengths = c(2, Inf)))
str(rmyTdm)
typeof(rmyTdm$dimnames$Terms)
findAssocs(rmyTdm, '자전거', 0.1)
inspect(rmyTdm[1:20, 1:20])
summary(data$m.dew_point)
install.packages("tm")
# install.packages("tm")
library(tm)
reV <- unlist(rules)
revec <- VectorSource(rules)
revecco <- Corpus(revec)
rmyTdm <- TermDocumentMatrix(revecco, control = list(wordLengths = c(2, Inf)))
str(rmyTdm)
typeof(rmyTdm$dimnames$Terms)
findAssocs(rmyTdm, '따릉이', 0.1)
inspect(rmyTdm[1:20, 1:20])
consumerKey <- "lntNcVVwlkrucA1lNh7OlKLEe"
consumerSecret <- "rqzU6Rpbsn0l0Z3xAf9E8wu6wy0CG2zGQR2lpwWP3vZxFbpglN"
requestURL <- "https://api.twitter.com/oauth/request_token"
authURL <- "https://api.twitter.com/oauth/authorize"
accessURL <- "https://api.twitter.com/oauth/access_token"
twitCred <- OAuthFactory$new(
consumerKey = consumerKey,
consumerSecret = consumerSecret,
requestURL = requestURL,
accessURL = accessURL,
authURL = authURL)
library(devtools)
install_github("twitteR", username = "geoffjentry")
library(twitteR)
setup_twitter_oauth("lntNcVVwlkrucA1lNh7OlKLEe",
"rqzU6Rpbsn0l0Z3xAf9E8wu6wy0CG2zGQR2lpwWP3vZxFbpglN",
access_token = "954249281702998016-OWZf5z6QywhqtogveBWPwFA0qWAjXqv",
access_secret = "NdKA7hVvFvsrgiTPKphGuS1wB6U3IEQUJWCTfRVJIM4TS")
n <- 2000
keyword <- '공유자전거'
keyword <- enc2utf8(keyword)
tweets2 <- searchTwitter(keyword, n)
Rangers_df <- do.call("rbind", lapply(tweets2, as.data.frame))
write.csv(Rangers_df, "bycle.csv")
tweet <- read.csv("bycle_final.csv")
tweet <- read.csv("bycle.csv")
#특수문자 삭제 @ 같은
# ?str_replace_all
# install.packages("stringr")
library(stringr)
tweet$text<-str_replace_all(tweet$text,"\\W"," ")
head(tweet$text)
# install.packages("KoNLP") #형태소 분석 패키지
# install.packages("wordcloud") #워드클라우드 패키지
# install.packages("RColorBrewer") #색상패키지
# Sys.setenv(JAVA_HOME='c:/Program Files/Java/jer1.8.0_161')
library(KoNLP)
library(wordcloud)
library(RColorBrewer)
useSejongDic()  #세종사전 활용하기
#명사추출
myword<-sapply(tweet$text, extractNoun, USE.NAMES = T)
myword
#list->unlist
gogo<-unlist(myword)
head(gogo)
#필요없는 단어 삭제하기
gogo<-gsub("\\d+","",gogo) #모든 숫자 없애기
gogo<-gsub("RT","",gogo)
gogo<-gsub("SSUL_","",gogo)
gogo<-gsub("최애에","",gogo)
gogo<-gsub(" ","",gogo)
gogo<-Filter(function(x){nchar(x)>1},gogo)
gogo<-sapply(gogo, function(x){Filter(function(y){nchar(y)<=8&&nchar(y)>1&&is.hangul(y)},x)})
#한글, 8자 이하만 필터링
#워드클라우드를 만들기 위해 단어만 수집된 텍스트 파일을 만드는 작업을 해준다
write(unlist(gogo),"screen_tweet.txt") #텍스트 파일로 저장
tweet_cloud<-read.table("screen_tweet.txt") #다시불러오기
head(tweet_cloud)
#table(분할표) 만들기
wordcount<-table(tweet_cloud) #단어카운트
wordcount<-gsub("민호","",wordcount) #필요없는단어 다시 삭제
wordcount<-gsub("염력","",wordcount) #필요없는단어 다시 삭제
#다시 파일 저장
write(unlist(gogo),"screen_tweet.txt")
tweet_cloud<-read.table("screen_tweet.txt") #다시 불러오기
wordcount<-table(tweet_cloud) #다시 table만들기
#워드클라우드생성
palete<-brewer.pal(10,"Set1") #색상 설정
wordcloud(names(wordcount),freq = wordcount, min.freq = 5, random.order=F, random.color=F, colors=palete)
# create word-document table
df2 <- read.csv("bycle.csv", stringsAsFactors = F)
library(ggmap)
install.packages("ggmap")
library(ggmap)
gc <- geocode("seoul")
gc
library(imputation)
install.packages("imputation")
# install.packages("imputation")
library(imputation)
install.packages("imputation")
install.packages("imputation")
getwd()
setwd("C:\\github\\proj_sponge_bicycle\\data_join\\final_dataset")
data <- read.csv("knn_imputation.csv", header = T)
dim(data);head(data);str(data);summary(data)
data <- data[,3:17]
head(data)
data <- data.frame(scale(data, scale = T, center = T))
head(data)
# modeling
set.seed(1)
# modeling
set.seed(1)
################################################################
# # Lasso for 10-fold cross-validation
# install.packages("glmnet")
library(glmnet)
################################################################
# # Lasso for 10-fold cross-validation
install.packages("glmnet")
x <- data[,2:7]
x
str(x)
x <- data[,2:17]
str(x)
getwd()
setwd("C:\\github\\proj_sponge_bicycle\\data_join\\final_dataset")
data <- read.csv("knn_imputation.csv", header = T)
dim(data);head(data);str(data);summary(data)
data <- data[,3:17]
data <- data.frame(scale(data, scale = T, center = T))
head(data)
# modeling
set.seed(1)
################################################################
# # Lasso for 10-fold cross-validation
# install.packages("glmnet")
library(glmnet)
x <- data[,2:17]
head(data)
getwd()
setwd("C:\\github\\proj_sponge_bicycle\\data_join\\final_dataset")
data <- read.csv("knn_imputation.csv", header = T)
dim(data);head(data);str(data);summary(data)
data <- data[,3:17]
data <- data.frame(scale(data, scale = T, center = T))
head(data)
# modeling
set.seed(1)
################################################################
# # Lasso for 10-fold cross-validation
# install.packages("glmnet")
library(glmnet)
x <- data[,2:15]
str(x)
y <- data[,1]
head(y)
str(y)
fit.lasso <- glmnet(as.matrix(x), y, family = "gaussian", alpha = 1)
cv.lasso <- cv.glmnet(as.matrix(x), y, nfolds = 10, family = "gaussian", alpha = 1, lambda = fit.lasso$lambda)
plot(cv.lasso)
abline(v=log(cv.lasso$lambda.min), col="red")
abline(v=log(cv.lasso$lambda.1se), col="blue")
legend("bottomright", legend = c("log(lambda.min)", "log(lambda.1se)"), col=c("red", "blue"), lwd = 2)
fit.lasso.param <- cv.lasso$lambda.min
# fit.lasso.param <- cv.lasso$lambda.1se
fit.lasso.tune <- glmnet(as.matrix(x), y, family = "gaussian", alpha = 1, lambda = fit.lasso.param)
coef(fit.lasso.tune)
lasso.pred <- predict(fit.lasso.tune, newx = as.matrix(x), type = "response")
lasso.error <- lasso.pred - y
sqrt(mean(lasso.error^2)) # RMSE of Lasso
################################################################################
# Ridge for 10-fold cross-validation
fit.ridge <- glmnet(as.matrix(x), y, family = "gaussian", alpha = 0)
cv.ridge <- cv.glmnet(as.matrix(x), y, nfolds = 10, family = "gaussian", alpha = 0, lambda = fit.ridge$lambda)
plot(cv.ridge)
abline(v=log(cv.ridge$lambda.min), col="red")
abline(v=log(cv.ridge$lambda.1se), col="blue")
legend("bottomright", legend = c("log(lambda.min)", "log(lambda.1se)"), col=c("red", "blue"), lwd = 2)
fit.ridge.param <- cv.ridge$lambda.min
# fit.ridge.param <- cv.ridge$lambda.1se
fit.ridge.tune <- glmnet(as.matrix(x), y, family = "gaussian", alpha = 0, lambda = fit.ridge.param)
coef(fit.ridge.tune)
ridge.pred <- predict(fit.ridge.tune, newx = as.matrix(x), type = "response")
ridge.error <- ridge.pred - y
sqrt(mean(ridge.error^2)) # RMSE of Ridge
####################################################################################
# Elastic Net for 10-fold cross-validation
fit.elastic <- glmnet(as.matrix(x), y, family = "gaussian", alpha = .5)
cv.elastic <- cv.glmnet(as.matrix(x), y, nfolds = 10, family = "gaussian", alpha = .5, lambda = fit.elastic$lambda)
plot(cv.elastic)
abline(v=log(cv.elastic$lambda.min), col="red")
abline(v=log(cv.elastic$lambda.1se), col="blue")
legend("bottomright", legend = c("log(lambda.min)", "log(lambda.1se)"), col=c("red", "blue"), lwd = 2)
fit.elastic.param <- cv.elastic$lambda.min
# fit.elastic.param <- cv.elastic$lambda.1se
fit.elastic.tune <- glmnet(as.matrix(x), y, family = "gaussian", alpha = .5, lambda = fit.elastic.param)
coef(fit.elastic.tune)
elastic.pred <- predict(fit.elastic.tune, newx = as.matrix(x), type = "response")
elastic.error <- elastic.pred - y
sqrt(mean(elastic.error^2)) # RMSE of Elastic Net
##########################################################################
# Random forest
# install.packages("randomForest")
library(randomForest)
##########################################################################
# Random forest
install.packages("randomForest")
head(data)
# ?randomForest
fit.rf <- randomForest(rentcnt ~ ., data = data, ntree = 100)
##########################################################################
# Random forest
# install.packages("randomForest")
library(randomForest)
# ?randomForest
fit.rf <- randomForest(rentcnt ~ ., data = data, ntree = 100)
# names(fit.rf)
rf.pred <- fit.rf$predicted
rf.error <- rf.pred - data[["rentcnt"]]
sqrt(mean(rf.error^2)) # RMSE of random forest
importance(fit.rf)
varImpPlot(fit.rf)
###########################################################################
# SVR for 10-fold cross-validation
# install.packages("e1071")
library(e1071)
###########################################################################
# SVR for 10-fold cross-validation
install.packages("e1071")
###########################################################################
# SVR for 10-fold cross-validation
# install.packages("e1071")
library(e1071)
# ??e1071
fit.svr <- svm(total_rental ~ ., kernel = "radial", cross = 10, data = data)
# ??e1071
fit.svr <- svm(rentcnt ~ ., kernel = "radial", cross = 10, data = data)
summary(fit.svr)
# names(fit.svr)
mean(sqrt(fit.svr$MSE)) # RMSE of SVR
###################################################
# Decision tree for 10-fold cross validation
# install.packages("tree")
library(tree)
###################################################
# Decision tree for 10-fold cross validation
install.packages("tree")
###################################################
# Decision tree for 10-fold cross validation
# install.packages("tree")
library(tree)
cv.folds <- function (n, folds = 10) {
split(sample(1:n), rep(1:folds, length = n))
}
(K <- 10)
# (K <- nrow(dat))
(all.folds <- cv.folds(nrow(data), K))
str(all.folds)
cv.RMSE <- NULL
fit.tree <- tree(rentcnt ~ ., data=data[-test,])
cv.RMSE <- NULL
cv.RMSE <- NULL
for(j in 1:K){
test  <- all.folds[[j]]
fit.tree <- tree(rentcnt ~ ., data=data[-test,])
tree.pred  <- predict(fit.tree, data[test,])
cv.RMSE[j] <- sqrt(sum((data[test,]$rentcnt - tree.pred)^2)/length(tree.pred))
}
list(cv.RMSE, mean(cv.RMSE)) # RMSE of decision tree
par(mfrow=c(1,2))
cv.fit.tree <- cv.tree(fit.tree, K = 10)
names(cv.fit.tree)
plot(cv.fit.tree$size, cv.fit.tree$dev, type = "b", main = "Optimal Tree Size")
prune.fit.tree <- prune.tree(fit.tree, best = 6)
prune.fit.tree
plot(prune.fit.tree)
text(prune.fit.tree)
###########################################################
# Multiple Linear Regression for 10-fold cross validation
cv.folds <- function (n, folds = 10) {
split(sample(1:n), rep(1:folds, length = n))
}
(K <- 10)
# (K <- nrow(dat))
(all.folds <- cv.folds(nrow(data), K))
str(all.folds)
cv.RMSE <- NULL
for(j in 1:K){
test  <- all.folds[[j]]
model <- lm(rentcnt ~ ., data=data[-test,])
pred  <- predict(model, data[test,])
cv.RMSE[j] <- sqrt(sum((data[test,]$total_rental - pred)^2)/length(pred))
}
list(cv.RMSE, mean(cv.RMSE)) # RMSE of Multiple Linear Regression
cv.RMSE <- NULL
for(j in 1:K){
test  <- all.folds[[j]]
model <- lm(rentcnt ~ ., data=data[-test,])
pred  <- predict(model, data[test,])
cv.RMSE[j] <- sqrt(sum((data[test,]$rentcnt - pred)^2)/length(pred))
}
list(cv.RMSE, mean(cv.RMSE)) # RMSE of Multiple Linear Regression
#########################################################
##Gradient Boosting##
library(gbm)
#########################################################
##Gradient Boosting##
install.packages("gbm")
#########################################################
##Gradient Boosting##
# install.packages("gbm")
library(gbm)
cv.folds <- function (n, folds = 10) {
split(sample(1:n), rep(1:folds, length = n))
}
(K <- 10)
# (K <- nrow(dat))
(all.folds <- cv.folds(nrow(data), K))
str(all.folds)
cv.RMSE <- NULL
for(j in 1:K){
test  <- all.folds[[j]]
model <- gbm(rentcnt ~ ., data=data[-test,])
pred  <- predict(model, data[test,])
cv.RMSE[j] <- sqrt(sum((data[test,]$rentcnt - pred)^2)/length(pred))
}
cv.RMSE <- NULL
for(j in 1:K){
test  <- all.folds[[j]]
model <- gbm(rentcnt ~ ., data=data[-test,], n.trees = 5000)
pred  <- predict(model, data[test,])
cv.RMSE[j] <- sqrt(sum((data[test,]$rentcnt - pred)^2)/length(pred))
}
cv.RMSE <- NULL
for(j in 1:K){
test  <- all.folds[[j]]
model <- gbm(rentcnt ~ ., data=data[-test,], distribution = "gaussian", n.trees = 5000)
pred  <- predict(model, data[test,])
cv.RMSE[j] <- sqrt(sum((data[test,]$rentcnt - pred)^2)/length(pred))
}
cv.RMSE <- NULL
for(j in 1:K){
test  <- all.folds[[j]]
model <- gbm(rentcnt ~ ., data=data[-test,], distribution = "gaussian", n.trees = 5000, shrinkage = 0.01, interaction.depth = 4)
pred  <- predict(model, data[test,])
cv.RMSE[j] <- sqrt(sum((data[test,]$rentcnt - pred)^2)/length(pred))
}
########## END ######################
?gbm
cv.RMSE <- NULL
for(j in 1:K){
test  <- all.folds[[j]]
model <- gbm(rentcnt ~ ., data=data[-test,], distribution = "gaussian", n.trees = 100, shrinkage = 0.01, interaction.depth = 4)
pred  <- predict(model, data[test,])
cv.RMSE[j] <- sqrt(sum((data[test,]$rentcnt - pred)^2)/length(pred))
}
model <- gbm(rentcnt ~ ., data=data[-test,], distribution = "gaussian", n.trees = 100, shrinkage = 0.01, interaction.depth = 4)
cv.RMSE[j] <- sqrt(sum((data[test,]$rentcnt - pred)^2)/length(pred))
cv.RMSE <- NULL
for(j in 1:K){
test  <- all.folds[[j]]
model <- gbm(rentcnt ~ ., data=data[-test,], distribution = "gaussian", n.trees = 100, shrinkage = 0.01, interaction.depth = 4)
pred  <- predict(model, data[test,])
cv.RMSE[j] <- sqrt(sum((data[test,]$rentcnt - pred)^2)/length(pred))
}
cv.RMSE <- NULL
for(j in 1:K){
test  <- all.folds[[j]]
model <- gbm(rentcnt ~ ., data=data[-test,], distribution = "gaussian", n.trees = 100, shrinkage = 0.01, interaction.depth = 4)
pred  <- predict.gbm(model, data[test,], n.trees = 100)
cv.RMSE[j] <- sqrt(sum((data[test,]$rentcnt - pred)^2)/length(pred))
}
list(cv.RMSE, mean(cv.RMSE))
cv.RMSE <- NULL
for(j in 1:K){
test  <- all.folds[[j]]
model <- gbm(rentcnt ~ ., data=data[-test,], distribution = "gaussian", n.trees = 5000, shrinkage = 0.01, interaction.depth = 4)
pred  <- predict.gbm(model, data[test,], n.trees = 5000)
cv.RMSE[j] <- sqrt(sum((data[test,]$rentcnt - pred)^2)/length(pred))
}
list(cv.RMSE, mean(cv.RMSE))
summary(model)
#########################################################
##knn##
library(FNN)
#########################################################
##knn##
install.packages("FNN")
#########################################################
##knn##
# install.packages("FNN")
library(FNN)
cv.folds <- function (n, folds = 10) {
split(sample(1:n), rep(1:folds, length = n))
}
(K <- 10)
# (K <- nrow(dat))
(all.folds <- cv.folds(nrow(data), K))
str(all.folds)
cv.RMSE <- NULL
for(j in 1:K){
test  <- all.folds[[j]]
model <- knn.reg(train = data[-test,], test = data[test,], y = data$rentcnt, k = 3, algorithm = "kd_tree")
pred  <- model$pred
cv.RMSE[j] <- sqrt(sum((data[test,]$rentcnt - pred)^2)/length(pred))
}
list(cv.RMSE, mean(cv.RMSE))
########## END ######################
model$pred
