library(stringr)
a <- "양념감자\n핡\n"
str_dup(a,10)
str_dup(a,10)
library(stringr)
a <- "양념감자\n핡\n"
library(stringr)
a <- "양념감자\n핡\n"
str_dup(a,10)
a <- "abcdefASFGASDF12345"
str_replace_all(a,'[A-z]','')
a
str_replace_all(a,'[a-Z]','')
a2 <- 'apple'+'/'+'is'
# 이거 안됨 a2 <- 'apple'+'/'+'is'
a2 <- str_c('apple','/','is')
# 이거 안됨 a2 <- 'apple'+'/'+'is'
a2 <- str_c('apple','/','is'); a2
# 이거 안됨 a2 <- 'apple'+'/'+'is'
a2 <- str_c('apple','/','is','/','a'); a2
str_split(a,'/')
str_split(a2,'/')
# 이거 안됨 a2 <- 'apple'+'/'+'is'
a2 <- str_c('   apple','/','is','/','a   '); a2
str_split(a2,'/')
a2
str_sub(a2, start=1, end= 3)
str_sub(a2, start=10, end= 13)
a3 <- str_split(a2,'/')
str_sub(a3, start=1, end= 3)
a3
str_sub(a3, start=1, end= 3)
str_sub(a3, start=1, end= 13)
library(googleVis)
fruit
Fruits
aggregate(Sales~Year, Fruits, min)
aggregate(Sales~Fruit, Fruits, median)
aggregate(Sales~Fruit+Year, Fruits, median)
cbind(aggregate(Sales~Fruit+Year, Fruits, median)
,aggregate(Sales~Fruit+Year, Fruits, mean)
)
cbind(aggregate(Sales~Fruit+Year, Fruits, median)
,aggregate(Sales~Fruit+Year, Fruits, sum)
)
aggregate(Sales~Year, Fruits, mean)
aggregate(Sales~Fruit, Fruits, median)
cbind(aggregate(Sales~Fruit+Year, Fruits, mean)
,aggregate(Sales~Fruit+Year, Fruits, median)
)
cbind(aggregate(Sales~Year, Fruits, mean)
,aggregate(Sales~Fruit, Fruits, median)
)
cbind(aggregate(Sales~Fruit, Fruits, mean)
,aggregate(Sales~Fruit, Fruits, median)
)
cbind(aggregate(Sales~Fruit, Fruits, mean)
,aggregate(Sales~Fruit, Fruits, median)$Sales
)
mat1 <- matrix(c(1,2,3,4,5,6), nrow=2)
mat1
mat1 <- matrix(c(1,2,3,4,5,6), nrow=2, ncol=3)
mat1
mat1 <- matrix(c(1,2,3,4,5,6), nrow=2)
mat1 <- matrix(c(1,2,3,4,5,6), nrow=2)
mat1
str_split(str1,"/")
str1 <- "a/p/c"
str_split(str1,"/")
library(stringr)
str_split(str1,"/")
typeof(str_split(str1,"/"))
str1 <- "  a/p/c  "
str_trim(str1, "left")
mat1 <- matrix(c(1,2,3,4,5,6), nrow=2, byrow= T)
mat1
mat1 <- matrix(c(1,2,3,4,5,6), nrow=2)
mat1
str_trim(str1, left)
str_trim(str1, "left")
str_replace_all('a??????abc???daa??','\?\?','?')
str_replace_all('a??????abc???daa??','??','?')
str_replace_all('a??????abc???daa??',"?+","?")
str_replace_all('a??????abc???daa??',\?+,"?")
str_replace_all('a??????abc???daa??',"\?+","?")
str_replace_all('a??????abc???daa??',?+,"?")
str_replace_all('a??????abc???daa??',"[?]+","?")
str_replace_all('a??????abc???daa??',"\?+","?")
str_replace_all('a??????abc???daa??',"\\?+","?")
str_replace_all('a??????abc???daa??',"\\?\\?","?")
str_replace_all('a??????abc???daa??',"(\\?)\\?+",'\\1')
str_replace_all('a??????abc???daa??',"\\?\\?","")
df1 <- as.data.frame(
abs_i= c(3,2,1,0,1,2,3),
i= c(3,2,1,0,-1,-2,-3),
c3= c(0,1,3,5,3,1,0),
n= c(7,7,7,7,7,7,7)
)
df1 <- as.data.frame(
abs_i= c(3,2,1,0,1,2,3),
i= c(3,2,1,0,-1,-2,-3),
c3= c(0,1,3,5,3,1,0),
n= c(7,7,7,7,7,7,7)
)
df1 <- data.frame(
abs_i= c(3,2,1,0,1,2,3),
i= c(3,2,1,0,-1,-2,-3),
c3= c(0,1,3,5,3,1,0),
n= c(7,7,7,7,7,7,7)
)
df1
lm(c3~abs_i+i+n)
lm(c3~abs_i+i+n, df1)
df2 <- data.frame(
abs_i= c(2,1,0,1,2),
i= c(2,1,0,-1,-2),
c3= c(1,3,5,3,1),
n= c(7,7,7,7,7)
)
df2
lm(c3~abs_i+i+n, df2)
lm(c3~abs_i, df2)
df2 <- data.frame(
abs_i= c(2,1,0,1,2),
i= c(2,1,0,-1,-2),
c3= c(1,3,5,3,1),
n= c(7,7,7,7,7)
)
df3_5
df3_5 <- data.frame(
abs_i= c(1,0,1),
i= c(1,0,-1),
c3= c(1,3,1),
n= c(5,5,5)
)
df3_5
lm(c3~., df3_5)
lm(c3~abs_i, df3_5)
lm(c3~., df2)
lm(c3~., df3_5)
## 신세계다 ㅋㅋㅋㅋㅋㅋㅋㅋ
## 빈 다이아 중간 공백 몇 개 출력해야 하느냐
## abs_i: abs(i), i: index, c3: 중간공백 수, n: 몇 줄짜리 다이아
df1 <- data.frame(
abs_i= c(3,2,1,0,1,2,3),
i= c(3,2,1,0,-1,-2,-3),
c3= c(0,1,3,5,3,1,0),
n= c(7,7,7,7,7,7,7)
)
df3_5 <- data.frame(
abs_i= c(1,0,1),
i= c(1,0,-1),
c3= c(1,3,1),
n= c(5,5,5)
)
lm(c3~., df2)
lm(c3~., df3_5)
lm(c3~abs_i, df2)
lm(c3~., df3_5)
lm(c3~abs_i, df3_5)
str_locate('anaconda','a')
library(stringr)
str_locate_all('anaconda','a')
str_locate('anaconda','a')
11*5+2*3
(11*5+2*3)*1.25
(11*5)*1.25
a <- ()
a <- []
a <- [1]
a <- c(1)
a[1]
a[3] <- 5
a
??str_detect_all
library(stringr)
??str_detect_all
??str_detect
??str_detect_all
??str_detect
??str_detect_all
setwd("E:/github/proj_sponge_bicycle/data")
#english - tm
install.packages("KoNLP") #extractNoun. ??치?念? jdk?却?
install.packages("RColorBrewer") #brewer.pal
install.packages("wordcloud") #wordcloud
library(KoNLP)
library(RColorBrewer)
library(wordcloud)
text_jb51_homepage <- readLines("cpNaverNewsContent.txt")
txtjb2 <- sapply(text_jb51_homepage, extractNoun, USE.NAMES=F)
txtjb3 <- unlist(txtjb2)
head(txtjb3,10)
txt <- readLines("w_ignore.txt")
txt[5]
cnt_txt <- length(txt)
for(i in 1:cnt_txt){
txtjb3 <- gsub((txt[i]),"",txtjb3)
}
txtjb3 <- Filter(function(x) {
nchar(x)>=2
}, txtjb3)
# save- I need
write(unlist(txtjb3),"wordCloud_before.txt")
# wordcloud- for viewing
data4 <- read.table("wordCloud_before.txt")
wordcount <- table(data4)
#占쏙옙?? ?? 20????
head(sort(wordcount, decreasing=T), 20)
palete <- brewer.pal(8,"Set2")
wordcloud(names(wordcount), freq= wordcount, scale=c(5,1)
,rot.per=0, min.freq=1, random.order=F
,random.color=T, colors=palete)
#占쏙옙?? ?? 20????
head(sort(wordcount, decreasing=T), 20)
palete <- brewer.pal(8,"Set2")
wordcloud(names(wordcount), freq= wordcount, scale=c(5,1)
,rot.per=0, min.freq=1, random.order=F
,random.color=T, colors=palete)
text_jb51_homepage <- readLines("cpNaverNewsContent.txt")
txtjb2 <- sapply(text_jb51_homepage, extractNoun, USE.NAMES=F)
txtjb3 <- unlist(txtjb2)
head(txtjb3,10)
txt <- readLines("w_ignore.txt")
txt[5]
cnt_txt <- length(txt)
for(i in 1:cnt_txt){
txtjb3 <- gsub((txt[i]),"",txtjb3)
}
txtjb3 <- Filter(function(x) {
nchar(x)>=2
}, txtjb3)
# save- I need
write(unlist(txtjb3),"wordCloud_before.txt")
# wordcloud- for viewing
data4 <- read.table("wordCloud_before.txt")
wordcount <- table(data4)
#占쏙옙?? ?? 20????
head(sort(wordcount, decreasing=T), 20)
palete <- brewer.pal(8,"Set2")
wordcloud(names(wordcount), freq= wordcount, scale=c(5,1)
,rot.per=0, min.freq=1, random.order=F
,random.color=T, colors=palete)
tweet <- read.csv("twitter.csv")
setwd("E:/github/proj_sponge_bicycle/data")
tweet <- read.csv("cpNaverNewsContent.csv")
#특수문자 삭제 @ 같은
# ?str_replace_all
# install.packages("stringr")
library(stringr)
tweet$text<-str_replace_all(tweet$text,"\\W"," ")
head(tweet$text)
# install.packages("KoNLP") #형태소 분석 패키지
# install.packages("wordcloud") #워드클라우드 패키지
# install.packages("RColorBrewer") #색상패키지
# Sys.setenv(JAVA_HOME='c:/Program Files/Java/jer1.8.0_161')
library(KoNLP)
library(wordcloud)
library(RColorBrewer)
useSejongDic()  #세종사전 활용하기
#명사추출
myword<-sapply(tweet$text, extractNoun, USE.NAMES = T)
myword
#list->unlist
gogo<-unlist(myword)
head(gogo)
#필요없는 단어 삭제하기
gogo<-gsub("\\d+","",gogo) #모든 숫자 없애기
gogo<-gsub("RT","",gogo)
gogo<-gsub("SSUL_","",gogo)
gogo<-gsub("최애에","",gogo)
gogo<-gsub(" ","",gogo)
gogo<-Filter(function(x){nchar(x)>1},gogo)
gogo<-sapply(gogo, function(x){Filter(function(y){nchar(y)<=8&&nchar(y)>1&&is.hangul(y)},x)})
#한글, 8자 이하만 필터링
#워드클라우드를 만들기 위해 단어만 수집된 텍스트 파일을 만드는 작업을 해준다
write(unlist(gogo),"screen_tweet.txt") #텍스트 파일로 저장
tweet_cloud<-read.table("screen_tweet.txt") #다시불러오기
setwd("E:/github/proj_sponge_bicycle/data")
tweet <- read.csv("cpNaverNewsContent.csv")
#특수문자 삭제 @ 같은
# ?str_replace_all
# install.packages("stringr")
library(stringr)
tweet$text<-str_replace_all(tweet$text,"\\W"," ")
head(tweet$text)
tweet$text<-str_replace_all(tweet,"\\W"," ")
useSejongDic()  #세종사전 활용하기
#명사추출
myword<-sapply(tweet, extractNoun, USE.NAMES = T)
myword
#list->unlist
gogo<-unlist(myword)
head(gogo)
tweet$text<-str_replace_all(tweet$text,"\\W"," ")
head(tweet$text)
