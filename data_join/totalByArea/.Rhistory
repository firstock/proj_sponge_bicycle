# twitter
library(twitteR)
library(ROAuth)
# twitter
library(twitteR)
library(ROAuth)
# twitter
install.packages("twitteR")
library(ROAuth)
library(plyr)
library(stringr)
library(RJSONIO)
library(RCurl)
library(ggplot2)
# twitter
# install.packages("twitteR")
install.packages(c("ROAuth", "plyr", "stringr", "RJSONIO", "RCUrl", "ggplot2"))
# twitter
# install.packages("twitteR")
# install.packages(c("ROAuth", "plyr", "stringr", "RJSONIO", "RCUrl", "ggplot2"))
library(twitteR)
library(ROAuth)
library(plyr)
library(stringr)
library(RJSONIO)
library(RCurl)
library(ggplot2)
consumerKey <- "lntNcVVwlkrucA1lNh7OlKLEe"
consumerSecret <- "rqzU6Rpbsn0l0Z3xAf9E8wu6wy0CG2zGQR2lpwWP3vZxFbpglN"
requestURL <- "https://api.twitter.com/oauth/request_token"
authURL <- "https://api.twitter.com/oauth/authorize"
accessURL <- "https://api.twitter.com/oauth/access_token"
twitCred <- OAuthFactory$new(
consumerKey = consumerKey,
consumerSecret = consumerSecret,
requestURL = requestURL,
accessURL = accessURL,
authURL = authURL)
library(devtools)
install_github("twitteR", username = "geoffjentry")
library(twitteR)
setup_twitter_oauth("lntNcVVwlkrucA1lNh7OlKLEe",
"rqzU6Rpbsn0l0Z3xAf9E8wu6wy0CG2zGQR2lpwWP3vZxFbpglN",
access_token = "954249281702998016-OWZf5z6QywhqtogveBWPwFA0qWAjXqv",
access_secret = "NdKA7hVvFvsrgiTPKphGuS1wB6U3IEQUJWCTfRVJIM4TS")
# twitter
# install.packages("twitteR")
# install.packages(c("ROAuth", "plyr", "stringr", "RJSONIO", "RCUrl", "ggplot2"))
library(twitteR)
library(ROAuth)
library(plyr)
library(stringr)
library(RJSONIO)
library(RCurl)
library(ggplot2)
consumerKey <- "lntNcVVwlkrucA1lNh7OlKLEe"
consumerSecret <- "rqzU6Rpbsn0l0Z3xAf9E8wu6wy0CG2zGQR2lpwWP3vZxFbpglN"
requestURL <- "https://api.twitter.com/oauth/request_token"
authURL <- "https://api.twitter.com/oauth/authorize"
accessURL <- "https://api.twitter.com/oauth/access_token"
twitCred <- OAuthFactory$new(
consumerKey = consumerKey,
consumerSecret = consumerSecret,
requestURL = requestURL,
accessURL = accessURL,
authURL = authURL)
library(devtools)
install_github("twitteR", username = "geoffjentry")
library(twitteR)
setup_twitter_oauth("lntNcVVwlkrucA1lNh7OlKLEe",
"rqzU6Rpbsn0l0Z3xAf9E8wu6wy0CG2zGQR2lpwWP3vZxFbpglN",
access_token = "954249281702998016-OWZf5z6QywhqtogveBWPwFA0qWAjXqv",
access_secret = "NdKA7hVvFvsrgiTPKphGuS1wB6U3IEQUJWCTfRVJIM4TS")
n <- 2000
keyword <- '따릉이'
keyword <- enc2utf8(keyword)
tweets2 <- searchTwitter(keyword, n)
Rangers_df <- do.call("rbind", lapply(tweets2, as.data.frame))
write.csv(Rangers_df, "bycle.csv")
tweet <- read.csv("bycle_final.csv")
#특수문자 삭제 @ 같은
# ?str_replace_all
# install.packages("stringr")
library(stringr)
tweet$text<-str_replace_all(tweet$text,"\\W"," ")
head(tweet$text)
# install.packages("KoNLP") #형태소 분석 패키지
# install.packages("wordcloud") #워드클라우드 패키지
# install.packages("RColorBrewer") #색상패키지
# Sys.setenv(JAVA_HOME='c:/Program Files/Java/jer1.8.0_161')
library(KoNLP)
library(wordcloud)
library(RColorBrewer)
useSejongDic()  #세종사전 활용하기
#명사추출
myword<-sapply(tweet$text, extractNoun, USE.NAMES = T)
myword
#list->unlist
gogo<-unlist(myword)
head(gogo)
#필요없는 단어 삭제하기
gogo<-gsub("\\d+","",gogo) #모든 숫자 없애기
gogo<-gsub("RT","",gogo)
gogo<-gsub("SSUL_","",gogo)
gogo<-gsub("최애에","",gogo)
gogo<-gsub(" ","",gogo)
gogo<-Filter(function(x){nchar(x)>1},gogo)
gogo<-sapply(gogo, function(x){Filter(function(y){nchar(y)<=8&&nchar(y)>1&&is.hangul(y)},x)})
#한글, 8자 이하만 필터링
#워드클라우드를 만들기 위해 단어만 수집된 텍스트 파일을 만드는 작업을 해준다
write(unlist(gogo),"screen_tweet.txt") #텍스트 파일로 저장
tweet_cloud<-read.table("screen_tweet.txt") #다시불러오기
head(tweet_cloud)
#table(분할표) 만들기
wordcount<-table(tweet_cloud) #단어카운트
install.packages("KoNLP") #형태소 분석 패키지
# install.packages("KoNLP") #형태소 분석 패키지
install.packages("wordcloud") #워드클라우드 패키지
# install.packages("KoNLP") #형태소 분석 패키지
install.packages("wordcloud") #워드클라우드 패키지
# install.packages("RColorBrewer") #색상패키지
# Sys.setenv(JAVA_HOME='c:/Program Files/Java/jer1.8.0_161')
library(KoNLP)
# install.packages("KoNLP") #형태소 분석 패키지
install.packages("wordcloud") #워드클라우드 패키지
# install.packages("KoNLP") #형태소 분석 패키지
# install.packages("wordcloud") #워드클라우드 패키지
install.packages("RColorBrewer") #색상패키지
install.packages("RColorBrewer")
# install.packages("KoNLP") #형태소 분석 패키지
# install.packages("wordcloud") #워드클라우드 패키지
install.packages("RColorBrewer") #색상패키지
install.packages("RColorBrewer")
# install.packages("KoNLP") #형태소 분석 패키지
# install.packages("wordcloud") #워드클라우드 패키지
# install.packages("RColorBrewer") #색상패키지
# Sys.setenv(JAVA_HOME='c:/Program Files/Java/jer1.8.0_161')
library(KoNLP)
library(wordcloud)
library(RColorBrewer)
useSejongDic()  #세종사전 활용하기
#명사추출
myword<-sapply(tweet$text, extractNoun, USE.NAMES = T)
myword
#list->unlist
gogo<-unlist(myword)
head(gogo)
#필요없는 단어 삭제하기
gogo<-gsub("\\d+","",gogo) #모든 숫자 없애기
gogo<-gsub("RT","",gogo)
gogo<-gsub("SSUL_","",gogo)
gogo<-gsub("최애에","",gogo)
gogo<-gsub(" ","",gogo)
gogo<-Filter(function(x){nchar(x)>1},gogo)
gogo<-sapply(gogo, function(x){Filter(function(y){nchar(y)<=8&&nchar(y)>1&&is.hangul(y)},x)})
#한글, 8자 이하만 필터링
#워드클라우드를 만들기 위해 단어만 수집된 텍스트 파일을 만드는 작업을 해준다
write(unlist(gogo),"screen_tweet.txt") #텍스트 파일로 저장
tweet_cloud<-read.table("screen_tweet.txt") #다시불러오기
head(tweet_cloud)
#table(분할표) 만들기
wordcount<-table(tweet_cloud) #단어카운트
wordcount<-gsub("민호","",wordcount) #필요없는단어 다시 삭제
wordcount<-gsub("염력","",wordcount) #필요없는단어 다시 삭제
#다시 파일 저장
write(unlist(gogo),"screen_tweet.txt")
tweet_cloud<-read.table("screen_tweet.txt") #다시 불러오기
wordcount<-table(tweet_cloud) #다시 table만들기
#워드클라우드생성
palete<-brewer.pal(10,"Set1") #색상 설정
wordcloud(names(wordcount),freq = wordcount, min.freq = 5, random.order=F, random.color=F, colors=palete)
# create word-document table
df2 <- read.csv("bycle.csv", stringsAsFactors = F)
str(df2)
head(df2)
colnames(df2)
rules <- sapply(df2$text, strsplit, " ", USE.NAMES = F)
# install.packages("tm")
library(tm)
reV <- unlist(rules)
revec <- VectorSource(rules)
revecco <- Corpus(revec)
rmyTdm <- TermDocumentMatrix(revecco, control = list(wordLengths = c(2, Inf)))
str(rmyTdm)
typeof(rmyTdm$dimnames$Terms)
findAssocs(rmyTdm, '자전거', 0.1)
inspect(rmyTdm[1:20, 1:20])
summary(data$m.dew_point)
install.packages("tm")
# install.packages("tm")
library(tm)
reV <- unlist(rules)
revec <- VectorSource(rules)
revecco <- Corpus(revec)
rmyTdm <- TermDocumentMatrix(revecco, control = list(wordLengths = c(2, Inf)))
str(rmyTdm)
typeof(rmyTdm$dimnames$Terms)
findAssocs(rmyTdm, '따릉이', 0.1)
inspect(rmyTdm[1:20, 1:20])
consumerKey <- "lntNcVVwlkrucA1lNh7OlKLEe"
consumerSecret <- "rqzU6Rpbsn0l0Z3xAf9E8wu6wy0CG2zGQR2lpwWP3vZxFbpglN"
requestURL <- "https://api.twitter.com/oauth/request_token"
authURL <- "https://api.twitter.com/oauth/authorize"
accessURL <- "https://api.twitter.com/oauth/access_token"
twitCred <- OAuthFactory$new(
consumerKey = consumerKey,
consumerSecret = consumerSecret,
requestURL = requestURL,
accessURL = accessURL,
authURL = authURL)
library(devtools)
install_github("twitteR", username = "geoffjentry")
library(twitteR)
setup_twitter_oauth("lntNcVVwlkrucA1lNh7OlKLEe",
"rqzU6Rpbsn0l0Z3xAf9E8wu6wy0CG2zGQR2lpwWP3vZxFbpglN",
access_token = "954249281702998016-OWZf5z6QywhqtogveBWPwFA0qWAjXqv",
access_secret = "NdKA7hVvFvsrgiTPKphGuS1wB6U3IEQUJWCTfRVJIM4TS")
n <- 2000
keyword <- '공유자전거'
keyword <- enc2utf8(keyword)
tweets2 <- searchTwitter(keyword, n)
Rangers_df <- do.call("rbind", lapply(tweets2, as.data.frame))
write.csv(Rangers_df, "bycle.csv")
tweet <- read.csv("bycle_final.csv")
tweet <- read.csv("bycle.csv")
#특수문자 삭제 @ 같은
# ?str_replace_all
# install.packages("stringr")
library(stringr)
tweet$text<-str_replace_all(tweet$text,"\\W"," ")
head(tweet$text)
# install.packages("KoNLP") #형태소 분석 패키지
# install.packages("wordcloud") #워드클라우드 패키지
# install.packages("RColorBrewer") #색상패키지
# Sys.setenv(JAVA_HOME='c:/Program Files/Java/jer1.8.0_161')
library(KoNLP)
library(wordcloud)
library(RColorBrewer)
useSejongDic()  #세종사전 활용하기
#명사추출
myword<-sapply(tweet$text, extractNoun, USE.NAMES = T)
myword
#list->unlist
gogo<-unlist(myword)
head(gogo)
#필요없는 단어 삭제하기
gogo<-gsub("\\d+","",gogo) #모든 숫자 없애기
gogo<-gsub("RT","",gogo)
gogo<-gsub("SSUL_","",gogo)
gogo<-gsub("최애에","",gogo)
gogo<-gsub(" ","",gogo)
gogo<-Filter(function(x){nchar(x)>1},gogo)
gogo<-sapply(gogo, function(x){Filter(function(y){nchar(y)<=8&&nchar(y)>1&&is.hangul(y)},x)})
#한글, 8자 이하만 필터링
#워드클라우드를 만들기 위해 단어만 수집된 텍스트 파일을 만드는 작업을 해준다
write(unlist(gogo),"screen_tweet.txt") #텍스트 파일로 저장
tweet_cloud<-read.table("screen_tweet.txt") #다시불러오기
head(tweet_cloud)
#table(분할표) 만들기
wordcount<-table(tweet_cloud) #단어카운트
wordcount<-gsub("민호","",wordcount) #필요없는단어 다시 삭제
wordcount<-gsub("염력","",wordcount) #필요없는단어 다시 삭제
#다시 파일 저장
write(unlist(gogo),"screen_tweet.txt")
tweet_cloud<-read.table("screen_tweet.txt") #다시 불러오기
wordcount<-table(tweet_cloud) #다시 table만들기
#워드클라우드생성
palete<-brewer.pal(10,"Set1") #색상 설정
wordcloud(names(wordcount),freq = wordcount, min.freq = 5, random.order=F, random.color=F, colors=palete)
# create word-document table
df2 <- read.csv("bycle.csv", stringsAsFactors = F)
library(ggmap)
install.packages("ggmap")
library(ggmap)
gc <- geocode("seoul")
gc
library(imputation)
install.packages("imputation")
# install.packages("imputation")
library(imputation)
install.packages("imputation")
install.packages("imputation")
getwd()
setwd("C:\\github\\proj_sponge_bicycle\\data_join\\totalByArea")
data <- read.csv("광진구.csv", header = T)
dim(data);head(data)
data <- data[, 3:26]
data <- data[,c(1:15, 17, 18, 22, 23)]
str(data)
data <- data.frame(scale(data, scale = T, center = T))
head(data);str(data);dim(data)
###################################################
# Decision tree for 10-fold cross validation
# install.packages("tree")
library(tree)
cv.folds <- function (n, folds = 10) {
split(sample(1:n), rep(1:folds, length = n))
}
(K <- 10)
# (K <- nrow(dat))
(all.folds <- cv.folds(nrow(data), K))
str(all.folds)
cv.RMSE <- NULL
for(j in 1:K){
test  <- all.folds[[j]]
fit.tree <- tree(rentcnt ~ ., data=data[-test,])
tree.pred  <- predict(fit.tree, data[test,])
cv.RMSE[j] <- sqrt(sum((data[test,]$rentcnt - tree.pred)^2)/length(tree.pred))
}
list(cv.RMSE, mean(cv.RMSE)) # RMSE of decision tree
cv.fit.tree <- cv.tree(fit.tree, K = 10)
names(cv.fit.tree)
plot(cv.fit.tree$size, cv.fit.tree$dev, type = "b", main = "Optimal Tree Size")
prune.fit.tree <- prune.tree(fit.tree, best = 11)
prune.fit.tree
plot(prune.fit.tree)
text(prune.fit.tree)
plot(prune.fit.tree, main = prune tree)
plot(prune.fit.tree, main = "prune tree")
text(prune.fit.tree)
getwd()
setwd("C:\\github\\proj_sponge_bicycle\\data_join\\totalByArea")
data <- read.csv("광진구.csv", header = T)
# Tuning Gradient Boosting
# install.packages("caret")
library(caret)
library(gbm)
grid<-expand.grid(.n.trees=seq(100,5000,by=200),.interaction.depth=seq(1,4,by=1),.shrinkage=c(.001,.01,.1),
.n.minobsinnode=10)
control<-trainControl(method = "CV")
gbm.train<-train(rentcnt~.,data=data,method='gbm',trControl=control)
getwd()
setwd("C:\\github\\proj_sponge_bicycle\\data_join\\totalByArea")
data <- read.csv("광진구.csv", header = T)
dim(data);head(data)
data <- data[, 3:26]
data <- data[,c(1:15, 17, 18, 22, 23)]
str(data)
data <- data.frame(scale(data, scale = T, center = T))
head(data);str(data);dim(data)
X <- data[,2:19]
y <- data[,1]
# Tuning Gradient Boosting
# install.packages("caret")
library(caret)
library(gbm)
grid<-expand.grid(.n.trees=seq(100,5000,by=200),.interaction.depth=seq(1,4,by=1),.shrinkage=c(.001,.01,.1),
.n.minobsinnode=10)
control<-trainControl(method = "CV")
gbm.train<-train(rentcnt~.,data=data,method='gbm',trControl=control)
gbm.train
#########################################################
##Gradient Boosting for 10-fold cross validation##
# install.packages("gbm")
library(gbm)
cv.folds <- function (n, folds = 10) {
split(sample(1:n), rep(1:folds, length = n))
}
(K <- 10)
# (K <- nrow(dat))
(all.folds <- cv.folds(nrow(data), K))
str(all.folds)
cv.RMSE <- NULL
for(j in 1:K){
test  <- all.folds[[j]]
model <- gbm(rentcnt ~ ., data=data[-test,], distribution = "gaussian", n.trees = 150, shrinkage = 0.1, interaction.depth = 3, n.minobsinnode = 10)
pred  <- predict.gbm(model, data[test,], n.trees = 150)
cv.RMSE[j] <- sqrt(sum((data[test,]$rentcnt - pred)^2)/length(pred))
}
list(cv.RMSE, mean(cv.RMSE))
summary(model)
